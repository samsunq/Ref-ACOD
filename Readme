Ref-ACOD is a referring segmentation dataset aim to artificial camouflaged objects. This dataset collected 6K figures, which conclued some military artificial camouflaged objects in the wild scenes. 

The benchmark statistics, Ref-ACOD, comprises a collection of 6,000 images, with the number of expressions allocated across the training, and validation sets being 5,681, 1168, respectively. Notably, a significant proportion of the target objects are extremely small, occupying only a tiny fraction of the overall image. Conversely, there are also large-scale objects whose sizes exceed 50% of the image area. This variability presents a challenging task, as the dataset requires models to predict targets within images that exhibit significant scale variation, including numerous small objects and
large-scale targets.

These military artificial camouflaged objects included military soldiers, tanks, armored vehicles, and military equipment with camouflage nets. To pursue the distinction of positions and categories in reference segmentation, we strive to ensure that multiple target units are present in each image we obtain. In response to the concept of referring segmentation and typical datasets used, such as ref-coco and ref-coco+, we created the Ref-ACOD dataset using a similar production method and data format as the refcoco dataset. 
This is to facilitate the subsequent use of this dataset by allowing users to directly read and process it using the same methods as the refcoco dataset. The details is shown as fellows:

![detection](https://github.com/samsunq/Ref-ACOD/blob/main/thecontractionforACOD.png)

1. The dataset includes bounding box annotations for all phrases; 
2. Ref-ACOD only splits the train/validation sets; 
3. Ref-ACOD contains absolute spatial terms, such as describing the position of an object in the lower right corner of the image;
4. The dataset has segmentation annotations for all objects, using the standard LabelMe format.

We propose a novel RIS benchmark dataset tailored for the COD task, named Referring Artificial Camouflaged Objects Detection (Ref-ACOD). Inspired by the outstanding segmentation performance of the Segment Anything Model (SAM), we adopt a semi-automated approach that utilizes bounding boxes and SAM to generate pixel-level masks, significantly reducing annotation costs. Specifically, we follow these steps to generate pixel-level
annotations for linguistic expressions:
‚Ä¢ Step 1: Using the bounding box prompts provided by the CPQA dataset, we employ SAM to generate pixel-level masks for all images in the dataset.
‚Ä¢ Step 2: The dataset is carefully curated to identify problematic data, and manual annotation is performed to produce masks that meet the annotation standards. This process is assisted by a software tool designed based on SAM principles, ensuring the precision of the masks that correspond to the linguistic expressions.
‚Ä¢ Step 3: To improve the compatibility of Ref-ACOD with RIS models, the finalized annotations are converted into the RefCOCO dataset format, enhancing usability.

thecontractionforACOD.png
üí° AttentionÔºÅ

This dataset contains a portion of real battlefield videos and image captures, which may cause discomfort to readers. Please be aware that this dataset is intended for COD detection of artificial camouflage targets aimed at security and military purposes. Additionally, some data in this dataset comes from publicly available videos and images on the internet. This dataset is for non-commercial use only. 
The author declares: The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.
